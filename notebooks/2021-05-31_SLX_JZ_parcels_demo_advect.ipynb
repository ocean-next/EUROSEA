{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: produce lagrangian particule trajectories  from eNATL60 with Ocean PArcels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a demo notebook of using thw software Ocean Parcels (https://oceanparcels.org/Lagrangian) to produce lagrangian trajectories from the eNATL60 1-h currents (u,v at 15 m depth).\n",
    "\n",
    "\n",
    "#### Objective of this notebook:\n",
    "* Initialize 146 particles, evenly spaced over the western mediterranean domain. \n",
    "* Advect for 1 month and save outputs.\n",
    "* 1h output frequency\n",
    "* Use 15-meter depth currents (u,v) from eNATL60-no-tide.\n",
    "* Sample u, v along the trajectory.\n",
    "* Restart another experiment from previous outputs if needed.\n",
    "\n",
    "\n",
    "#### Misc. notes:\n",
    "* An extraction of the eNatl60 data over the Western Mediterranean region has been applied ahead of this notebook. We are loading these extracted files below.\n",
    "* A 1-month advection experiment -> 20 min and no memory problems (the number of particles or the addition of u,v sampled fields don't change the computing time much).\n",
    "* I noticed that running a second experiment in  the same open notebook leads to memory problems (some sort of memory leakage?) on Jean Zay. So each 1-month experiment should be run in a newly opened notebook (shutdown in between experiments).\n",
    "* u,v sampled along the trajectories are initially in rad/sec and must be converted to m/s in the end (see at the end of this notebook)\n",
    "\n",
    "#### Machine:\n",
    "* This notebook has run on the french HPCJeanZay@IDRIS, on the prepost partition (FREE CPU up to 2h runtime, and more memory per node)\n",
    "    ``` \n",
    "    salloc --account egi@cpu --ntasks=1 --cpus-per-task 10 --hint=nomultithread --partition prepost   --time=03:00:00 srun --pty bash \n",
    "    ```\n",
    "    Then:\n",
    "    ```\n",
    "    module load parcels/2.2.1\n",
    "    idrjup\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standart libraries\n",
    "import os,sys\n",
    "import numpy as np\n",
    "\n",
    "# xarray\n",
    "import xarray as xr\n",
    "\n",
    "# plot\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Colormap\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.colors import from_levels_and_colors\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "\n",
    "# custom tools for plotting\n",
    "import lib_medwest60 as slx\n",
    "\n",
    "\n",
    "# for ocean parcels\n",
    "from parcels import FieldSet, ParticleSet, Variable, JITParticle, AdvectionRK4, ParticleFile,ErrorCode\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import pytest\n",
    "from glob import glob\n",
    "from datetime import timedelta as delta\n",
    "from os import path\n",
    "import time\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "\n",
    "# for jupyter notebook display\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region to process (MEDWEST in our case)\n",
    "rbox='MEDWEST'\n",
    "\n",
    "# experiment number\n",
    "exp=2\n",
    "\n",
    "# new particle set or restarted from existing particle file\n",
    "newset=True\n",
    "\n",
    "# output directory for plots and drifters file\n",
    "diro=\"/gpfswork/rech/egi/regi915/PLT/EUROSEA_DRIFTERS/sandrine/prod/exp2/\"\n",
    "\n",
    "# input directory (for eNATL60 data files)\n",
    "data_path = '/gpfsstore/rech/egi/commun/EUROSEA/eNATL60-BLB002/'\n",
    "\n",
    "\n",
    "# u,v,ssh files\n",
    "ufiles = sorted(glob(data_path+'eNATL60'+rbox+'-BLB002_y2009m*.1h_vozocrtx_15m.nc'))\n",
    "vfiles = sorted(glob(data_path+'eNATL60'+rbox+'-BLB002_y2009m*.1h_vomecrty_15m.nc'))\n",
    "sshfiles = sorted(glob(data_path+'eNATL60'+rbox+'-BLB002_y2009m*.1h_sossheig.nc'))\n",
    "\n",
    "# grid files\n",
    "meshfi = 'mesh_hgr_eNATL60'+rbox+'_3.6.nc'\n",
    "maskfile   =data_path+'/mask_eNATL60'+rbox+'_3.6.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  Ocean Parcels reads the model data (and creates the \"fieldset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Casting depth data to np.float32\n",
      "WARNING: Trying to initialize a shared grid with different chunking sizes - action prohibited. Replacing requested field_chunksize with grid's master chunksize.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now creates the Ocean PArcels \"fieldset\" :\n",
    "# (see Ocean Parcels docs: \n",
    "# https://nbviewer.jupyter.org/github/OceanParcels/parcels/blob/master/parcels/examples/tutorial_nemo_curvilinear.ipynb)\n",
    "filenames = {'U': {'lon': data_path + meshfi,\n",
    "                   'lat': data_path + meshfi,\n",
    "                   'data': ufiles},\n",
    "             'V': {'lon': data_path + meshfi,\n",
    "                   'lat': data_path + meshfi,\n",
    "                   'data': vfiles}  }  #,\n",
    "           # 'SSH': {'lon': data_path + meshfi,\n",
    "           #        'lat': data_path + meshfi,\n",
    "           #        'data': sshfiles},}\n",
    "\n",
    "variables = {'U': 'vozocrtx', 'V': 'vomecrty'}#, 'SSH': 'sossheig'}\n",
    "\n",
    "dimensions = {'lon': 'glamf', 'lat': 'gphif', 'time': 'time_counter'}\n",
    "\n",
    "# Fieldset definition:\n",
    "field_set = FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:  Create the particle set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output and restart file names (filo and filorst)\n",
    "filo=rbox+\"-EUROSEA_146p_30d_freq1h_start_10-2009_exp\"+str(exp)\n",
    " # idlen is the number of days in the month over which to run the experiment, basically 30 or 31 (or 28)\n",
    "idlen=31\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a new particle class where u and v are also sampled* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleParticle(JITParticle):         # Define a new particle class where u and v are also sampled\n",
    "    #ssh = Variable('ssh')  \n",
    "    uinterp = Variable('uinterp')  \n",
    "    vinterp = Variable('vinterp')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create initial, evenly spaced, set of particle locations:\n",
    "\n",
    "Here we pick up evenly spaced (longitudes,latitudes) from the model grid and excluding land points.\n",
    "To increase the coverage, increase the increment incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "if (seg == 0):\n",
    "    # pick up lon lat from the model grid (evenly spaced) in the ocean (excluding land points)\n",
    "    incr=45\n",
    "    nlatp = nav_lat_refU.where(((maskU>0.)&(maskV>0.)))[8:803:incr,2:883:incr]\n",
    "    nlonp = nav_lon_refU.where(((maskU>0.)&(maskV>0.)))[8:803:incr,2:883:incr]\n",
    "\n",
    "    # make sure we are excuding the same points in both arrays\n",
    "    nlatp = nlatp.where(nlonp)\n",
    "    nlonp = nlonp.where(nlatp)\n",
    "\n",
    "    # stack x, y in one dimension\n",
    "    nlatp_st = nlatp.stack(z=(\"x\", \"y\"))\n",
    "    nlonp_st = nlonp.stack(z=(\"x\", \"y\"))\n",
    "\n",
    "    # drop nan values to get a list of lon lat to feed parcels\n",
    "    nlatp_st_nona = nlatp_st.dropna(dim='z')\n",
    "    nlonp_st_nona = nlonp_st.dropna(dim='z')\n",
    "\n",
    "    # print size (i.e. number of particles)\n",
    "    print(nlatp_st_nona.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create particle set (either new evenly spaced locations or restarted from previous file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newset:\n",
    "    # IF NEW PARTICLE SET:\n",
    "    # Create the particle set by sampling the field set according to the lat,lon list created above\n",
    "    npart = nlatp_st_nona.size  # number of particles to be released\n",
    "    td = xr.full_like(nlatp_st_nona,delta(days=31+31+30).total_seconds())   # release every particle one hour later\n",
    "    pset = ParticleSet.from_list(fieldset=field_set, pclass=SampleParticle, lon=nlonp_st_nona, lat=nlatp_st_nona,time=td)\n",
    "\n",
    "else:\n",
    "    # IF RESTART FROM EXISTING PARTICLE SET:\n",
    "    pset = ParticleSet.from_particlefile(fieldset=field_set, pclass=SampleParticle, filename=diro+filorst+'.nc', restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:  Advect the particle set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* customization functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function that samples U and V at particle location\n",
    "def SampleUV(particle, fieldset, time):  \n",
    "    (u1, v1) = fieldset.UV[time, particle.depth,particle.lat, particle.lon]\n",
    "    particle.uinterp = u1\n",
    "    particle.vinterp = v1\n",
    "    \n",
    "# create a function that will kill particles when they go out of the regional boundaries\n",
    "# see https://nbviewer.jupyter.org/github/OceanParcels/parcels/blob/master/parcels/examples/tutorial_Agulhasparticles.ipynb\n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* set kernels to apply to the particle set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic RK4 advection kernel\n",
    "k_ADVRK4 = pset.Kernel(AdvectionRK4)\n",
    "\n",
    "# additional kernel to sample u,v    \n",
    "k_sampleUV = pset.Kernel(SampleUV)    # Casting the SampleP function to a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* advect the particles for 1 month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: u1 declared in multiple Kernels\n",
      "WARNING: v1 declared in multiple Kernels\n",
      "INFO: Compiled SampleParticleAdvectionRK4SampleUV ==> /tmp/parcels-43033/f72a8274eaa5c5462153af608437d190_0.so\n",
      "INFO: Temporary output files are stored in /gpfswork/rech/egi/regi915/PLT/EUROSEA_DRIFTERS/sandrine/prod/exp2/out-RQKUFQME.\n",
      "INFO: You can use \"parcels_convert_npydir_to_netcdf /gpfswork/rech/egi/regi915/PLT/EUROSEA_DRIFTERS/sandrine/prod/exp2/out-RQKUFQME\" to convert these to a NetCDF file during the run.\n",
      " 59% (1594800.0 of 2678400.0) |#####     | Elapsed Time: 0:05:08 ETA:   0:08:52"
     ]
    }
   ],
   "source": [
    "# start time of the computation idlen\n",
    "start = time.time()\n",
    "\n",
    "# define the characteristics of the particle file (the trajectories)\n",
    "pfile = ParticleFile(diro+filo, pset, outputdt=delta(hours=1))\n",
    "\n",
    "# compute with the kernel defined above\n",
    "#pset.execute(k_ADVRK4+k_sampleSSH+k_sampleUV, runtime=delta(days=6), dt=delta(hours=0.25), output_file=pfile)\n",
    "pset.execute(k_ADVRK4+k_sampleUV, runtime=delta(days=idlen), dt=delta(hours=1), output_file=pfile,recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle})\n",
    "\n",
    "# display time of computation\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "# save output to netcdf\n",
    "pfile.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: s|ee next notebooks to convert and plot the resuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
